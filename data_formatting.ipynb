{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.558405Z",
     "start_time": "2024-11-19T19:33:06.535311Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only uncomment loading and saving if we want to overwrite dataset.hf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18c2235f82fa05b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"es\", trust_remote_code=True,\n",
    "#            storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}}, num_proc= 8, split=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.559950Z",
     "start_time": "2024-11-19T19:33:06.552363Z"
    }
   },
   "id": "e06b290a10973d74",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"dataset.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.580949Z",
     "start_time": "2024-11-19T19:33:06.563275Z"
    }
   },
   "id": "c22cce59dccd01b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"dataset.hf\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(train_size=0.8, seed=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.587925Z",
     "start_time": "2024-11-19T19:33:06.567048Z"
    }
   },
   "id": "27cc5fc063f20969",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only uncomment if we want to retrain the Mbart tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d07674210119d31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"es_XX\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.591759Z",
     "start_time": "2024-11-19T19:33:07.588245Z"
    }
   },
   "id": "cf3de26e13381422",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# max_length = 512\n",
    "# \n",
    "# def preprocess_text(text):\n",
    "#     inputs = [txt[\"en\"] for txt in text[\"translation\"]]\n",
    "#     targets = [txt[\"es\"] for txt in text[\"translation\"]]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "#     )\n",
    "#     return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.596317Z",
     "start_time": "2024-11-19T19:33:07.592252Z"
    }
   },
   "id": "a0c4838f018e9f50",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenized_datasets = split_dataset.map(\n",
    "#     preprocess_text,\n",
    "#     batched=True,\n",
    "#     remove_columns=split_dataset[\"train\"].column_names,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.597011Z",
     "start_time": "2024-11-19T19:33:07.594621Z"
    }
   },
   "id": "ab3152e3bbd659d",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#tokenized_datasets.save_to_disk(\"tokenized_dataset.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.601902Z",
     "start_time": "2024-11-19T19:33:07.598075Z"
    }
   },
   "id": "b04d5e51b3a51d02",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcogracie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.606177Z",
     "start_time": "2024-11-19T19:33:07.601526Z"
    }
   },
   "id": "4df9cd28d3ca3e19",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/49147400 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72ece96ecb1f4f24aeaae4c0db3dfb01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding linear regression tokenizer\n",
    "def tokenize_batch(batch):\n",
    "    return {\"tokenized\": [word_tokenize(txt[\"en\"]) for txt in batch[\"translation\"]]}\n",
    "\n",
    "# Apply tokenization to the dataset and save\n",
    "tokenized_train = split_dataset[\"train\"].map(tokenize_batch, batched=True)\n",
    "tokenized_train.to_csv(\"encoded_inputs_train.csv\")\n",
    "\n",
    "tokenized_test = split_dataset[\"test\"].map(tokenize_batch, batched=True)\n",
    "tokenized_test.to_csv(\"encoded_inputs_test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-19T19:33:07.605640Z"
    }
   },
   "id": "f3995851b68a28c0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# loading csvs\n",
    "# with open(\"encoded_inputs_train.csv\", \"r\") as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     encoded_inputs_train = [list(map(int, row)) for row in reader]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4d3d36bf02e58e24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
