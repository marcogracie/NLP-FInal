{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.558405Z",
     "start_time": "2024-11-19T19:33:06.535311Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only uncomment loading and saving if we want to overwrite dataset.hf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18c2235f82fa05b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"es\", trust_remote_code=True,\n",
    "#            storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}}, num_proc= 8, split=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.559950Z",
     "start_time": "2024-11-19T19:33:06.552363Z"
    }
   },
   "id": "e06b290a10973d74",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"dataset.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:06.580949Z",
     "start_time": "2024-11-19T19:33:06.563275Z"
    }
   },
   "id": "c22cce59dccd01b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"dataset.hf\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(train_size=0.8, seed=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.587925Z",
     "start_time": "2024-11-19T19:33:06.567048Z"
    }
   },
   "id": "27cc5fc063f20969",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only uncomment if we want to retrain the Mbart tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d07674210119d31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"es_XX\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.591759Z",
     "start_time": "2024-11-19T19:33:07.588245Z"
    }
   },
   "id": "cf3de26e13381422",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# max_length = 512\n",
    "# \n",
    "# def preprocess_text(text):\n",
    "#     inputs = [txt[\"en\"] for txt in text[\"translation\"]]\n",
    "#     targets = [txt[\"es\"] for txt in text[\"translation\"]]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "#     )\n",
    "#     return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.596317Z",
     "start_time": "2024-11-19T19:33:07.592252Z"
    }
   },
   "id": "a0c4838f018e9f50",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenized_datasets = split_dataset.map(\n",
    "#     preprocess_text,\n",
    "#     batched=True,\n",
    "#     remove_columns=split_dataset[\"train\"].column_names,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.597011Z",
     "start_time": "2024-11-19T19:33:07.594621Z"
    }
   },
   "id": "ab3152e3bbd659d",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#tokenized_datasets.save_to_disk(\"tokenized_dataset.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.601902Z",
     "start_time": "2024-11-19T19:33:07.598075Z"
    }
   },
   "id": "b04d5e51b3a51d02",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marcogracie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:33:07.606177Z",
     "start_time": "2024-11-19T19:33:07.601526Z"
    }
   },
   "id": "4df9cd28d3ca3e19",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/49147400 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72ece96ecb1f4f24aeaae4c0db3dfb01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenized\u001B[39m\u001B[38;5;124m\"\u001B[39m: [word_tokenize(txt[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m txt \u001B[38;5;129;01min\u001B[39;00m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m\"\u001B[39m]]}\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Apply tokenization to the dataset and save\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m tokenized_train \u001B[38;5;241m=\u001B[39m \u001B[43msplit_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m tokenized_train\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoded_inputs_train.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m tokenized_test \u001B[38;5;241m=\u001B[39m split_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(tokenize_batch, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    558\u001B[0m }\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 560\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    561\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3055\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3050\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3051\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3052\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3053\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3054\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3055\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[1;32m   3056\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3057\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3458\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3454\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m   3455\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[1;32m   3456\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[1;32m   3457\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3458\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3462\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3463\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3464\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[1;32m   3465\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[1;32m   3466\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3467\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3320\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[1;32m   3318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[1;32m   3319\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[0;32m-> 3320\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[1;32m   3322\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   3323\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[1;32m   3324\u001B[0m     }\n",
      "Cell \u001B[0;32mIn[20], line 3\u001B[0m, in \u001B[0;36mtokenize_batch\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_batch\u001B[39m(batch):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenized\u001B[39m\u001B[38;5;124m\"\u001B[39m: [word_tokenize(txt[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m txt \u001B[38;5;129;01min\u001B[39;00m \u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtranslation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m]}\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:279\u001B[0m, in \u001B[0;36mLazyDict.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    277\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[key]\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys_to_format:\n\u001B[0;32m--> 279\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeys_to_format\u001B[38;5;241m.\u001B[39mremove(key)\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:382\u001B[0m, in \u001B[0;36mLazyBatch.format\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m--> 382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_column\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpa_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:448\u001B[0m, in \u001B[0;36mPythonFormatter.format_column\u001B[0;34m(self, pa_table)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m--> 448\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpython_arrow_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    449\u001B[0m     column \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_features_decoder\u001B[38;5;241m.\u001B[39mdecode_column(column, pa_table\u001B[38;5;241m.\u001B[39mcolumn_names[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m column\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:148\u001B[0m, in \u001B[0;36mPythonArrowExtractor.extract_column\u001B[0;34m(self, pa_table)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_column\u001B[39m(\u001B[38;5;28mself\u001B[39m, pa_table: pa\u001B[38;5;241m.\u001B[39mTable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m--> 148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpa_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pylist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/pyarrow/table.pxi:1327\u001B[0m, in \u001B[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/pyarrow/array.pxi:1605\u001B[0m, in \u001B[0;36mpyarrow.lib.Array.to_pylist\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/site-packages/pyarrow/scalar.pxi:777\u001B[0m, in \u001B[0;36mpyarrow.lib.StructScalar.as_py\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlp-env/lib/python3.10/_collections_abc.py:885\u001B[0m, in \u001B[0;36mKeysView.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    882\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mapping\n\u001B[0;32m--> 885\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    886\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mapping\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Adding linear regression tokenizer\n",
    "def tokenize_batch(batch):\n",
    "    return {\"tokenized\": [word_tokenize(txt[\"en\"]) for txt in batch[\"translation\"]]}\n",
    "\n",
    "# Apply tokenization to the dataset and save\n",
    "tokenized_train = split_dataset[\"train\"].map(tokenize_batch, batched=True)\n",
    "tokenized_train.to_csv(\"encoded_inputs_train.csv\")\n",
    "\n",
    "tokenized_test = split_dataset[\"test\"].map(tokenize_batch, batched=True)\n",
    "tokenized_test.to_csv(\"encoded_inputs_test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:36:17.705905Z",
     "start_time": "2024-11-19T19:33:07.605640Z"
    }
   },
   "id": "f3995851b68a28c0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# loading csvs\n",
    "# with open(\"encoded_inputs_train.csv\", \"r\") as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     encoded_inputs_train = [list(map(int, row)) for row in reader]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-19T19:36:17.705817Z"
    }
   },
   "id": "4d3d36bf02e58e24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
